{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pymorphy2\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import re\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Загрузка необходимых ресурсов\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Инициализация pymorphy2 для лемматизации\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Получаем список стоп-слов для русского языка из NLTK\n",
        "stop_words = set(stopwords.words('russian')).union({\n",
        "    \"как\", \"есть\", \"иногда\", \"нравится\", \"это\", \"а\", \"и\", \"в\", \"на\", \"что\", \"с\",\n",
        "    \"бы\", \"чтобы\", \"да\", \"нет\", \"ага\", \"ну\", \"во\", \"по\", \"всё\", \"точно\", \"прикольно\",\n",
        "    \"блин\", \"хрен\", \"чертов\", \"зараза\", \"хрень\", \"черт\", \"фига\", \"фигачи\", \"срань\",\n",
        "    \"что\", \"что-то\", \"для\", \"чё\", \"во\", \"чтобы\", \"чтоб\", \"?\", \",\", \".\", \"!\", \"1\",\n",
        "    \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"\n",
        "})\n",
        "\n",
        "# Функция для очистки текста от знаков препинания и лишних символов\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[\\d\\W]', ' ', text)  # Удаляем все символы, кроме букв и пробелов\n",
        "    text = text.lower()  # Приводим к нижнему регистру\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Заменяем несколько пробелов на один\n",
        "    return text.strip()  # Удаляем пробелы в начале и конце строки\n",
        "\n",
        "# Функция для лемматизации\n",
        "def lemmatize(sentence):\n",
        "    cleaned_sentence = clean_text(sentence)  # Очищаем текст\n",
        "    tokens = word_tokenize(cleaned_sentence)  # Токенизация\n",
        "    lemmatized_tokens = []\n",
        "    for token in tokens:\n",
        "        token_lower = token.lower()  # Приведение к нижнему регистру\n",
        "        if token_lower not in stop_words:  # Фильтрация стоп-слов\n",
        "            parsed_word = morph.parse(token_lower)[0]  # Лемматизация\n",
        "            normal_form = parsed_word.normal_form\n",
        "            lemmatized_tokens.append(normal_form)  # Добавляем лемму слова\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Чтение CSV-файла и лемматизация\n",
        "def process_csv(file_path):\n",
        "    processed_sentences = []\n",
        "    with open(file_path, newline=\"\", encoding='utf-8') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # Пропускаем заголовок\n",
        "        for row in reader:\n",
        "            sentence = row[0]  # Извлекаем предложение\n",
        "            processed_sentence = lemmatize(sentence)  # Лемматизируем предложение\n",
        "            processed_sentences.append(processed_sentence)\n",
        "    return processed_sentences\n",
        "\n",
        "# Загрузка модели Sentence Transformers\n",
        "def get_sentence_embeddings(sentences):\n",
        "    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')  # Легковесная модель\n",
        "    embeddings = model.encode(sentences, convert_to_numpy=True)\n",
        "    return embeddings\n",
        "\n",
        "# Функция для кластеризации и вывода результатов с использованием KMeans\n",
        "def cluster_sentences_kmeans(sentences, n_clusters):\n",
        "    # Получаем эмбеддинги предложений\n",
        "    embeddings = get_sentence_embeddings(sentences)\n",
        "\n",
        "    # Кластеризация методом KMeans\n",
        "    kmeans = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "    # Вывод кластеров и создание облаков слов\n",
        "    generate_wordclouds_and_histograms(sentences, labels)\n",
        "\n",
        "\n",
        "# Функция для извлечения существительных из предложений\n",
        "def extract_nouns(sentences):\n",
        "    nouns = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence)\n",
        "        for token in tokens:\n",
        "            parsed_word = morph.parse(token)[0]\n",
        "            if 'NOUN' in parsed_word.tag:  # Проверяем, является ли слово существительным\n",
        "                nouns.append(parsed_word.normal_form)\n",
        "    return nouns\n",
        "\n",
        "# Функция для генерации облаков слов и гистограмм для каждого кластера\n",
        "def generate_wordclouds_and_histograms(sentences, labels):\n",
        "    clusters = {}\n",
        "    for sentence, label in zip(sentences, labels):\n",
        "        if label not in clusters:\n",
        "            clusters[label] = []\n",
        "        clusters[label].append(sentence)\n",
        "\n",
        "    # Определяем количество кластеров и создаем сетку подграфиков\n",
        "    num_clusters = len(clusters)\n",
        "    cols = 2  # Количество колонок для облаков слов и гистограмм\n",
        "    rows = num_clusters  # Количество строк зависит от количества кластеров\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))  # Размер всей сетки\n",
        "\n",
        "    # Убираем оси, если кластеров меньше, чем ячеек на графике\n",
        "    axes = axes.reshape(rows, cols) if num_clusters > 1 else [[axes]]\n",
        "\n",
        "    # Создание облаков слов и гистограмм для каждого кластера\n",
        "    for i, (label, cluster_sentences) in enumerate(clusters.items()):\n",
        "        nouns = extract_nouns(cluster_sentences)  # Извлечение существительных\n",
        "        top_nouns = Counter(nouns).most_common(10)  # Выбор 10 самых распространенных слов\n",
        "\n",
        "        # Генерация текста для облака слов\n",
        "        wordcloud_text = ' '.join([noun for noun, _ in top_nouns])\n",
        "\n",
        "        # Создание облака слов\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(wordcloud_text)\n",
        "\n",
        "        # Отображение облака слов на левом подграфике\n",
        "        axes[i][0].imshow(wordcloud, interpolation=\"bilinear\")\n",
        "        axes[i][0].axis(\"off\")\n",
        "        axes[i][0].set_title(f\"Word Cloud for Cluster {label}\")\n",
        "\n",
        "        # Данные для гистограммы\n",
        "        words, counts = zip(*top_nouns)\n",
        "\n",
        "        # Создание гистограммы на правом подграфике\n",
        "        axes[i][1].barh(words, counts, color='skyblue')\n",
        "        axes[i][1].set_xlabel(\"Frequency\")\n",
        "        axes[i][1].set_title(f\"Top Words for Cluster {label}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Путь к файлу CSV\n",
        "csv_file_path = 'output.csv'\n",
        "\n",
        "# Обработка предложений\n",
        "processed_sentences = process_csv(csv_file_path)\n",
        "\n",
        "# Кластеризация и визуализация с использованием KMeans\n",
        "cluster_sentences_kmeans(processed_sentences, n_clusters=4)  # Укажите нужное количество кластеров\n"
      ],
      "metadata": {
        "id": "S_aHnCN99c1s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
